{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing\n",
    "In this notebook, the goal is to process the dataset so that the dataset is ready to be used for training. To achieve this goal, here are the steps that will be performed in this notebook:\n",
    "1. Combine the CSV files of the dataset into one dataset\n",
    "2. Preliminary analysis of the dataset\n",
    "3. Dataset cleaning - clean missing values, duplicates, etc\n",
    "4. Further processing, includes addressing the high class imbalance problem and rename the columns name (if necessary)\n",
    "5. Save the processed dataset\n",
    "\n",
    "note: The notebook will be seperated into two subsections, where the training dataset and the testing dataset will be processed separately. However, same steps will be applied to both dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages to be used\n",
    "import os       # to create directories and remove files\n",
    "import csv      # to remove extra columns in CSE-CIC-IDS2018 dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# set the random seed to ensure the result is reproducible\n",
    "random.seed(10)\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIC-IDS2017 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Combine the files of the dataset\n",
    "\n",
    "In this step we will also address an issue where the files of the `CIC-IDS2017` dataset includes the replacement character (U+FFFD). This is because the dataset use the \"–\" character (Unicode code: U+2013), which cannot be processed by the Python-Pandas library. The replacement character will be replaced with the \"-\" character (Unicode code:45) when the files are being combined. The issue is addressed now instead of after importing the dataset as Dataframe because the dataset is very huge in size and the Pandas library is not optimized for such operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the name of all files into a tuple\n",
    "dataset = 'CIC-IDS2017'\n",
    "csv_file_names = ('Friday-WorkingHours-Afternoon-DDos.pcap_ISCX', \n",
    "                'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX',\n",
    "                'Friday-WorkingHours-Morning.pcap_ISCX',\n",
    "                'Monday-WorkingHours.pcap_ISCX',\n",
    "                'Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX',\n",
    "                'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX',\n",
    "                'Tuesday-WorkingHours.pcap_ISCX',\n",
    "                'Wednesday-workingHours.pcap_ISCX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to combine seperate files of a dataset\n",
    "# Besides combining the files, it has two additional functionality:\n",
    "# 1. Replace the replacement character (\\uFFFD) with '-', which is needed for the CIC-IDS2017 dataset\n",
    "# 2. Down sample the dataset. If the reduce_sample_size parameter is set to true, \n",
    "#    only 10% of the dataset will be randomly selected and saved. \n",
    "def combine_csv_files(dataset: str, file_names: tuple, reduce_sample_size: bool = False):\n",
    "\n",
    "    # create a new directory to place the combined dataset\n",
    "    os.makedirs('./Dataset/dataset_combined', exist_ok=True)\n",
    "\n",
    "    # remove the dataset if it already exist\n",
    "    merged_dataset_directory = f'Dataset/dataset_combined/{dataset}.csv'\n",
    "    if os.path.isfile(merged_dataset_directory):\n",
    "        os.remove(merged_dataset_directory)\n",
    "        print(f'original file({merged_dataset_directory}) has been removed')\n",
    "\n",
    "    for (file_index, file_name) in enumerate(file_names):\n",
    "        with open(f\"Dataset/{dataset}/{file_name}.csv\", 'r') as file, open(merged_dataset_directory, 'a') as out_file:\n",
    "            for (line_index, line) in enumerate(file):\n",
    "                \n",
    "                # only the header of the first file will be taken\n",
    "                if 'Label' in line or 'label' in line:\n",
    "                    if file_index != 0 or line_index != 0:\n",
    "                        continue\n",
    "                elif reduce_sample_size:\n",
    "                    if random.randint(1, 10) > 1:\n",
    "                        continue\n",
    "                # replace the replacement character (\\uFFFD) with '-' if exist     \n",
    "                out_file.write(line.replace(' ï¿½ ', '-'))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original file(Dataset/dataset_combined/CIC-IDS2017.csv) has been removed\n"
     ]
    }
   ],
   "source": [
    "combine_csv_files(dataset=dataset, file_names=csv_file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Preliminary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Destination Port</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packets</th>\n",
       "      <th>Total Backward Packets</th>\n",
       "      <th>Total Length of Fwd Packets</th>\n",
       "      <th>Total Length of Bwd Packets</th>\n",
       "      <th>Fwd Packet Length Max</th>\n",
       "      <th>Fwd Packet Length Min</th>\n",
       "      <th>Fwd Packet Length Mean</th>\n",
       "      <th>Fwd Packet Length Std</th>\n",
       "      <th>...</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>Active Mean</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54865</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55054</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55055</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46236</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54863</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Destination Port   Flow Duration   Total Fwd Packets  \\\n",
       "0              54865               3                   2   \n",
       "1              55054             109                   1   \n",
       "2              55055              52                   1   \n",
       "3              46236              34                   1   \n",
       "4              54863               3                   2   \n",
       "\n",
       "    Total Backward Packets  Total Length of Fwd Packets  \\\n",
       "0                        0                           12   \n",
       "1                        1                            6   \n",
       "2                        1                            6   \n",
       "3                        1                            6   \n",
       "4                        0                           12   \n",
       "\n",
       "    Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
       "0                             0                       6   \n",
       "1                             6                       6   \n",
       "2                             6                       6   \n",
       "3                             6                       6   \n",
       "4                             0                       6   \n",
       "\n",
       "    Fwd Packet Length Min   Fwd Packet Length Mean   Fwd Packet Length Std  \\\n",
       "0                       6                      6.0                     0.0   \n",
       "1                       6                      6.0                     0.0   \n",
       "2                       6                      6.0                     0.0   \n",
       "3                       6                      6.0                     0.0   \n",
       "4                       6                      6.0                     0.0   \n",
       "\n",
       "   ...   min_seg_size_forward  Active Mean   Active Std   Active Max  \\\n",
       "0  ...                     20          0.0          0.0            0   \n",
       "1  ...                     20          0.0          0.0            0   \n",
       "2  ...                     20          0.0          0.0            0   \n",
       "3  ...                     20          0.0          0.0            0   \n",
       "4  ...                     20          0.0          0.0            0   \n",
       "\n",
       "    Active Min  Idle Mean   Idle Std   Idle Max   Idle Min   Label  \n",
       "0            0        0.0        0.0          0          0  BENIGN  \n",
       "1            0        0.0        0.0          0          0  BENIGN  \n",
       "2            0        0.0        0.0          0          0  BENIGN  \n",
       "3            0        0.0        0.0          0          0  BENIGN  \n",
       "4            0        0.0        0.0          0          0  BENIGN  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put all csv files into a single dataframe\n",
    "cic_ids2017 = pd.read_csv('Dataset/dataset_combined/CIC-IDS2017.csv')\n",
    "cic_ids2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2830743\n",
      "Number of columns: 79\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {cic_ids2017.shape[0]}\")\n",
    "print(f\"Number of columns: {cic_ids2017.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([' Destination Port', ' Flow Duration', ' Total Fwd Packets',\n",
       "       ' Total Backward Packets', 'Total Length of Fwd Packets',\n",
       "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max',\n",
       "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean',\n",
       "       ' Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
       "       ' Bwd Packet Length Min', ' Bwd Packet Length Mean',\n",
       "       ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s',\n",
       "       ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min',\n",
       "       'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max',\n",
       "       ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std',\n",
       "       ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags',\n",
       "       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length',\n",
       "       ' Bwd Header Length', 'Fwd Packets/s', ' Bwd Packets/s',\n",
       "       ' Min Packet Length', ' Max Packet Length', ' Packet Length Mean',\n",
       "       ' Packet Length Std', ' Packet Length Variance', 'FIN Flag Count',\n",
       "       ' SYN Flag Count', ' RST Flag Count', ' PSH Flag Count',\n",
       "       ' ACK Flag Count', ' URG Flag Count', ' CWE Flag Count',\n",
       "       ' ECE Flag Count', ' Down/Up Ratio', ' Average Packet Size',\n",
       "       ' Avg Fwd Segment Size', ' Avg Bwd Segment Size',\n",
       "       ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk',\n",
       "       ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk',\n",
       "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes',\n",
       "       ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
       "       ' Init_Win_bytes_backward', ' act_data_pkt_fwd',\n",
       "       ' min_seg_size_forward', 'Active Mean', ' Active Std', ' Active Max',\n",
       "       ' Active Min', 'Idle Mean', ' Idle Std', ' Idle Max', ' Idle Min',\n",
       "       ' Label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Columns in the dataset:\")\n",
    "cic_ids2017.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the class distribution, we can clearly see that the CIC-IDS2017 dataset is very imbalance. The benign samples takes up a big portion of the dataset while some malicious samples like web attack contribute to a very small portion of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BENIGN                      2273097\n",
       "DoS Hulk                     231073\n",
       "PortScan                     158930\n",
       "DDoS                         128027\n",
       "DoS GoldenEye                 10293\n",
       "FTP-Patator                    7938\n",
       "SSH-Patator                    5897\n",
       "DoS slowloris                  5796\n",
       "DoS Slowhttptest               5499\n",
       "Bot                            1966\n",
       "Web Attack-Brute Force         1507\n",
       "Web Attack-XSS                  652\n",
       "Infiltration                     36\n",
       "Web Attack-Sql Injection         21\n",
       "Heartbleed                       11\n",
       "Name:  Label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Class distribution:')\n",
    "cic_ids2017[' Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution (normalized):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BENIGN                      80.300366\n",
       "DoS Hulk                     8.162981\n",
       "PortScan                     5.614427\n",
       "DDoS                         4.522735\n",
       "DoS GoldenEye                0.363615\n",
       "FTP-Patator                  0.280421\n",
       "SSH-Patator                  0.208320\n",
       "DoS slowloris                0.204752\n",
       "DoS Slowhttptest             0.194260\n",
       "Bot                          0.069452\n",
       "Web Attack-Brute Force       0.053237\n",
       "Web Attack-XSS               0.023033\n",
       "Infiltration                 0.001272\n",
       "Web Attack-Sql Injection     0.000742\n",
       "Heartbleed                   0.000389\n",
       "Name:  Label, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Class distribution (normalized):')\n",
    "cic_ids2017[' Label'].value_counts()/cic_ids2017.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows contain null value: \n",
      "Flow Bytes/s    1358\n",
      "dtype: int64\n",
      "\n",
      "Rows contain null value (percentage): \n",
      "Flow Bytes/s    0.047973\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cic_ids2017_null_count = cic_ids2017.isnull().sum()\n",
    "cic_ids2017_null_count = cic_ids2017_null_count[cic_ids2017_null_count > 0]\n",
    "print(f\"Rows contain null value: \\n{cic_ids2017_null_count}\\n\")\n",
    "\n",
    "cic_ids2017_null_count = cic_ids2017_null_count / cic_ids2017.shape[0] * 100\n",
    "print(f\"Rows contain null value (percentage): \\n{cic_ids2017_null_count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for infinity value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples contains infinity value:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2867"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of samples contains infinity value:')\n",
    "np.isinf(cic_ids2017.iloc[:, :-2]).any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for columns that contain string values\n",
    "\n",
    "Check for columns that is type `object`, which indicate the columns contain string value. The aim is to find if any column contain numeric and alphabetic value. Such column often include string value like '?' to indicate missing value, thus needed to be cleaned. \n",
    "\n",
    "In the CIC-IDS2017 dataset, only the Label column includes string value, hence no further cleaning will be needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Label    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cic_ids2017.dtypes[(cic_ids2017.dtypes != 'int64') & (cic_ids2017.dtypes != 'float64')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicated column\n",
    "\n",
    "Although there are no two columns with the same name, there are actually two columns for `Fwd Header Length`. By manually inspecting all of the columns, we can find that there is one column called `Fwd Header Length` while there is another column called `Fwd Header Length.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for duplicated column\n",
    "cic_ids2017.columns[cic_ids2017.columns.value_counts() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([' Destination Port', ' Flow Duration', ' Total Fwd Packets',\n",
       "       ' Total Backward Packets', 'Total Length of Fwd Packets',\n",
       "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max',\n",
       "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean',\n",
       "       ' Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
       "       ' Bwd Packet Length Min', ' Bwd Packet Length Mean',\n",
       "       ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s',\n",
       "       ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min',\n",
       "       'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max',\n",
       "       ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std',\n",
       "       ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags',\n",
       "       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length',\n",
       "       ' Bwd Header Length', 'Fwd Packets/s', ' Bwd Packets/s',\n",
       "       ' Min Packet Length', ' Max Packet Length', ' Packet Length Mean',\n",
       "       ' Packet Length Std', ' Packet Length Variance', 'FIN Flag Count',\n",
       "       ' SYN Flag Count', ' RST Flag Count', ' PSH Flag Count',\n",
       "       ' ACK Flag Count', ' URG Flag Count', ' CWE Flag Count',\n",
       "       ' ECE Flag Count', ' Down/Up Ratio', ' Average Packet Size',\n",
       "       ' Avg Fwd Segment Size', ' Avg Bwd Segment Size',\n",
       "       ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk',\n",
       "       ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk',\n",
       "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes',\n",
       "       ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
       "       ' Init_Win_bytes_backward', ' act_data_pkt_fwd',\n",
       "       ' min_seg_size_forward', 'Active Mean', ' Active Std', ' Active Max',\n",
       "       ' Active Min', 'Idle Mean', ' Idle Std', ' Idle Max', ' Idle Min',\n",
       "       ' Label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show all columns of the dataset to manually inspect it\n",
    "cic_ids2017.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308381, 79)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cic_ids2017_duplicates = cic_ids2017[cic_ids2017.duplicated()]\n",
    "cic_ids2017_duplicates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can see that the CIC-IDS2017 dataset contains quite a lot of duplicated entries. After looking into the number of duplicates according to the class of the samples, we have found that the duplicates are not specific to any class. Although 11% of duplicates is quite a lot, but these duplicates will be removed in the next step. The reason is that, 1. the dataset is large, even if we remove those samples, we still have sufficient samples to train our models. 2. duplicates will cause the models to bias towards them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.89% of rows are duplicates\n"
     ]
    }
   ],
   "source": [
    "print(f\"{cic_ids2017_duplicates.shape[0]/cic_ids2017.shape[0]*100:.2f}% of rows are duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BENIGN                    176613\n",
       "PortScan                   68111\n",
       "DoS Hulk                   58224\n",
       "SSH-Patator                 2678\n",
       "FTP-Patator                 2005\n",
       "DoS slowloris                411\n",
       "DoS Slowhttptest             271\n",
       "Web Attack-Brute Force        37\n",
       "Bot                           13\n",
       "DDoS                          11\n",
       "DoS GoldenEye                  7\n",
       "Name:  Label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cic_ids2017_duplicates[' Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Dataset cleaning\n",
    "\n",
    "From the analysis in Step 2, it has been discovered that the dataset contains a small amount of entries contain infinity value or missing value. These entries will be removed as they only account for a small portion of the dataset. \n",
    "\n",
    "Besides that, it has been discovered that 11% of the dataset are duplicates. Despite the duplicates account for a big portion of the dataset, they are still removed from the dataset. \n",
    "\n",
    "In terms of the column of the dataset, there is one duplicated column, which will be dropped in this step. Besides that, some of the column names contain an extra space as the first character of the column name. The extra space will also be addressed here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2827876, 79)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove rows with missing value or infinity value\n",
    "# by temporarily treating infinity value as null and use the dropna() function\n",
    "with pd.option_context('mode.use_inf_as_na', True):\n",
    "    cic_ids2017 = cic_ids2017.dropna(how='any')\n",
    "\n",
    "cic_ids2017.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2520798, 79)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cic_ids2017 = cic_ids2017.drop_duplicates()\n",
    "cic_ids2017.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2520798, 78)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the duplicated column\n",
    "cic_ids2017 = cic_ids2017.drop(' Fwd Header Length.1', axis=1)\n",
    "cic_ids2017.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Destination Port', 'Flow Duration', 'Total Fwd Packets',\n",
       "       'Total Backward Packets', 'Total Length of Fwd Packets',\n",
       "       'Total Length of Bwd Packets', 'Fwd Packet Length Max',\n",
       "       'Fwd Packet Length Min', 'Fwd Packet Length Mean',\n",
       "       'Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
       "       'Bwd Packet Length Min', 'Bwd Packet Length Mean',\n",
       "       'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s',\n",
       "       'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min',\n",
       "       'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max',\n",
       "       'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std',\n",
       "       'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags',\n",
       "       'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length',\n",
       "       'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s',\n",
       "       'Min Packet Length', 'Max Packet Length', 'Packet Length Mean',\n",
       "       'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count',\n",
       "       'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
       "       'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio',\n",
       "       'Average Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size',\n",
       "       'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate',\n",
       "       'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate',\n",
       "       'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets',\n",
       "       'Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
       "       'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward',\n",
       "       'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\n",
       "       'Idle Std', 'Idle Max', 'Idle Min', 'Label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the extra space in the columns' name\n",
    "cic_ids2017_columns = [column for column in cic_ids2017.columns]\n",
    "for column_index, column in enumerate(cic_ids2017_columns):\n",
    "    if column[0] == ' ':\n",
    "        cic_ids2017_columns[column_index] = column[1:]\n",
    "\n",
    "cic_ids2017.columns = cic_ids2017_columns\n",
    "cic_ids2017.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Dataset preparation\n",
    "\n",
    "In this step, the main goal is to address the high class imbalance problem. To reduce the imbalance problem, major classes will be down sampled. Besides that, all attack samples will be combined. Hence, the dataset will only contain two label, 'benign' and 'malicious'. Besides that, 'benign' samples will be downsampled so that the ratio between the number of benign samples and malicious samples will be $1:1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def downsample_dataset(dataset: pd.DataFrame, sample_count_per_class: pd.Series, max_sample: int) -> pd.DataFrame:\n",
    "    dataset_downsampled = pd.DataFrame()\n",
    "    for label, count in sample_count_per_class.items():\n",
    "\n",
    "        # set the upper bound\n",
    "        if count > max_sample:\n",
    "            sample_size = max_sample\n",
    "        else:\n",
    "            sample_size = count\n",
    "\n",
    "        sample = dataset[dataset['Label'] == label].sample(n=sample_size).reset_index(drop=True)\n",
    "        dataset_downsampled = pd.concat([dataset_downsampled, sample])\n",
    "    \n",
    "    return dataset_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BENIGN                      2095057\n",
       "DoS Hulk                     172846\n",
       "DDoS                         128014\n",
       "PortScan                      90694\n",
       "DoS GoldenEye                 10286\n",
       "FTP-Patator                    5931\n",
       "DoS slowloris                  5385\n",
       "DoS Slowhttptest               5228\n",
       "SSH-Patator                    3219\n",
       "Bot                            1948\n",
       "Web Attack-Brute Force         1470\n",
       "Web Attack-XSS                  652\n",
       "Infiltration                     36\n",
       "Web Attack-Sql Injection         21\n",
       "Heartbleed                       11\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the number of sample for each class\n",
    "sample_count_per_class = cic_ids2017['Label'].value_counts()\n",
    "sample_count_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of class after downsampling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BENIGN                      324881\n",
       "DDoS                        100000\n",
       "DoS Hulk                    100000\n",
       "PortScan                     90694\n",
       "DoS GoldenEye                10286\n",
       "FTP-Patator                   5931\n",
       "DoS slowloris                 5385\n",
       "DoS Slowhttptest              5228\n",
       "SSH-Patator                   3219\n",
       "Bot                           1948\n",
       "Web Attack-Brute Force        1470\n",
       "Web Attack-XSS                 652\n",
       "Infiltration                    36\n",
       "Web Attack-Sql Injection        21\n",
       "Heartbleed                      11\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cic_ids2017_attack = downsample_dataset(cic_ids2017, sample_count_per_class[1:], max_sample=100000)\n",
    "cic_ids2017_benign = cic_ids2017[cic_ids2017['Label'] == 'BENIGN'].sample(n=cic_ids2017_attack.shape[0]).reset_index(drop=True)\n",
    "cic_ids2017_downsampled = pd.concat([cic_ids2017_attack, cic_ids2017_benign])\n",
    "del cic_ids2017_attack\n",
    "del cic_ids2017_benign\n",
    "\n",
    "print('Distribution of class after downsampling')\n",
    "cic_ids2017_downsampled['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution (normalized):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BENIGN                      50.000000\n",
       "DDoS                        15.390251\n",
       "DoS Hulk                    15.390251\n",
       "PortScan                    13.958034\n",
       "DoS GoldenEye                1.583041\n",
       "FTP-Patator                  0.912796\n",
       "DoS slowloris                0.828765\n",
       "DoS Slowhttptest             0.804602\n",
       "SSH-Patator                  0.495412\n",
       "Bot                          0.299802\n",
       "Web Attack-Brute Force       0.226237\n",
       "Web Attack-XSS               0.100344\n",
       "Infiltration                 0.005540\n",
       "Web Attack-Sql Injection     0.003232\n",
       "Heartbleed                   0.001693\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Class distribution (normalized):')\n",
    "cic_ids2017_downsampled['Label'].value_counts()/cic_ids2017_downsampled.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relabel the dataset\n",
    "\n",
    "After relabeling the dataset, there will only be two classes, `malicious` and `benign`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "benign       324881\n",
       "malicious    324881\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cic_ids2017_downsampled.iloc[cic_ids2017_downsampled['Label'] != 'BENIGN', -1] = 'malicious'\n",
    "cic_ids2017_downsampled.iloc[cic_ids2017_downsampled['Label'] == 'BENIGN', -1] = 'benign'\n",
    "\n",
    "cic_ids2017_downsampled['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save the cleaned dataset\n",
    "def save_cleaned_dataset(dataframe: pd.DataFrame,dataset: str, tag: str = \"\"):\n",
    "    # create a new directory to save the cleaned dataset\n",
    "    os.makedirs('./Dataset/dataset_cleaned', exist_ok=True)\n",
    "\n",
    "    if not(tag == \"\"):\n",
    "        tag = \"_\" + tag\n",
    "\n",
    "    dataframe.to_csv(f'Dataset/dataset_cleaned/{dataset}{tag}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_cleaned_dataset(dataframe=cic_ids2017_downsampled, dataset='CIC-IDS2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE-CIC-IDS2018 Dataset\n",
    "\n",
    "The followings are the process to pre-process the CSE-CIC-IDS2018 dataset. The overall process is basically the same as that of the CIC-IDS2017 dataset. However, there are a few extra processing that is needed for the CSE-CIC-IDS2018 dataset. \n",
    "\n",
    "The most obvious extra processing is the need to align the columns of the CSE-CIC-IDS2018 dataset with that of the CIC-IDS2017 dataset. Hence, we will need to rename the column names of the 2018 dataset and ensure the sequence of the columns is exactly the same on both dataset. \n",
    "\n",
    "Moreover, the files of the 2018 dataset is more problematic. There are two main problems in the files:\n",
    "1. In some of the files, there are duplicated header within one file. \n",
    "2. In the `Tuesday-20-02-2018_TrafficForML_CICFlowMeter.csv` file, there are 84 columns instead of 80 as of other files. \n",
    "\n",
    "The fist problem will be addressed when merging the files into one single CSV file by using the self-define `combine_csv_files()` function. The function will address the problem by ignoring all header except the first line of the first file. \n",
    "\n",
    "The second problem will be addressed in Step 0 by inspecting which 4 columns are extra and those columns will be removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Clean the extra columns in the Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a normal file\n",
    "\n",
    "Load a normal file so that we can know which columns are there in files with 80 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 80)\n"
     ]
    }
   ],
   "source": [
    "# since our interest is to get the columns in the file, so we only load a small number of rows\n",
    "cse_cic_ids2018 = pd.read_csv(f'Dataset/CSE-CIC-IDS2018/Friday-02-03-2018_TrafficForML_CICFlowMeter.csv', nrows=10)\n",
    "cse_cic_ids2018_typical_columns = pd.Series(cse_cic_ids2018.columns, dtype='str')\n",
    "print(cse_cic_ids2018.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the problematic file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 84)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cse_cic_ids2018_20022018 = pd.read_csv('Dataset/CSE-CIC-IDS2018/Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv', nrows=10)\n",
    "cse_cic_ids2018_20022018_columns = pd.Series(cse_cic_ids2018_20022018.columns, dtype='str')\n",
    "\n",
    "cse_cic_ids2018_20022018.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for columns that only exist in the problematic file. \n",
    "\n",
    "From the result below, we find that there are four extra columns that only exist in the problematic file. Besides that, the index of these columns shows that they are the first four columns in the file. As there are only four extra columns, it means that there are no missing columns in the problematic file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Flow ID\n",
       "1      Src IP\n",
       "2    Src Port\n",
       "3      Dst IP\n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cse_cic_ids2018_20022018_columns[~cse_cic_ids2018_20022018_columns.isin(cse_cic_ids2018_typical_columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address the issue\n",
    "\n",
    "For performance reason, we use the CSV library to drop the first four columns of each row and save it to a new csv file. At the same time, let us fix the typo \"Thuesday\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dataset/CSE-CIC-IDS2018/Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv', 'r') as source, \\\n",
    "    open('Dataset/CSE-CIC-IDS2018/Tuesday-20-02-2018_TrafficForML_CICFlowMeter_dropped_first_four_columns.csv', 'w') as result:\n",
    "\n",
    "    original_dataset = csv.reader(source)\n",
    "    writer = csv.writer(result)\n",
    "\n",
    "    for row in original_dataset:\n",
    "        # exclude the first 4 columns when writing the file\n",
    "        writer.writerow((row[4:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the processed file to ensure the problem has been addressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 80)\n",
      "Index(['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts',\n",
      "       'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max',\n",
      "       'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std',\n",
      "       'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean',\n",
      "       'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean',\n",
      "       'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot',\n",
      "       'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n",
      "       'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
      "       'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags',\n",
      "       'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s',\n",
      "       'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean',\n",
      "       'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt',\n",
      "       'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt',\n",
      "       'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg',\n",
      "       'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg',\n",
      "       'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg',\n",
      "       'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts',\n",
      "       'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts',\n",
      "       'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts',\n",
      "       'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max',\n",
      "       'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "cse_cic_ids2018_20022018 = pd.read_csv('Dataset/CSE-CIC-IDS2018/Tuesday-20-02-2018_TrafficForML_CICFlowMeter_dropped_first_four_columns.csv', nrows=10)\n",
    "print(cse_cic_ids2018_20022018.shape)\n",
    "print(cse_cic_ids2018_20022018.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Combine the files of the dataset\n",
    "\n",
    "When combining the CSE-CIC-IDS2018 dataset, only 10% of the dataset will be used. This is because the 2018 dataset is too large to be handled by the hardware used in this work. Besides that, the CSE-CIC-IDS2018 dataset is only used for testing, 10% of the dataset is more than enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'CSE-CIC-IDS2018'\n",
    "dataset_csv_files = ('Friday-02-03-2018_TrafficForML_CICFlowMeter',\n",
    "                    'Friday-16-02-2018_TrafficForML_CICFlowMeter',\n",
    "                    'Friday-23-02-2018_TrafficForML_CICFlowMeter',\n",
    "                    'Tuesday-20-02-2018_TrafficForML_CICFlowMeter_dropped_first_four_columns',\n",
    "                    'Thursday-01-03-2018_TrafficForML_CICFlowMeter',\n",
    "                    'Thursday-15-02-2018_TrafficForML_CICFlowMeter',\n",
    "                    'Thursday-22-02-2018_TrafficForML_CICFlowMeter',\n",
    "                    'Wednesday-14-02-2018_TrafficForML_CICFlowMeter',\n",
    "                    'Wednesday-21-02-2018_TrafficForML_CICFlowMeter',\n",
    "                    'Wednesday-28-02-2018_TrafficForML_CICFlowMeter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original file(Dataset/dataset_combined/CSE-CIC-IDS2018.csv) has been removed\n"
     ]
    }
   ],
   "source": [
    "combine_csv_files(dataset, dataset_csv_files, reduce_sample_size=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Preliminary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dst Port</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Tot Fwd Pkts</th>\n",
       "      <th>Tot Bwd Pkts</th>\n",
       "      <th>TotLen Fwd Pkts</th>\n",
       "      <th>TotLen Bwd Pkts</th>\n",
       "      <th>Fwd Pkt Len Max</th>\n",
       "      <th>Fwd Pkt Len Min</th>\n",
       "      <th>...</th>\n",
       "      <th>Fwd Seg Size Min</th>\n",
       "      <th>Active Mean</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49684</td>\n",
       "      <td>6</td>\n",
       "      <td>02/03/2018 08:47:38</td>\n",
       "      <td>281</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>02/03/2018 08:47:41</td>\n",
       "      <td>250</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>02/03/2018 08:47:40</td>\n",
       "      <td>60860062</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>870.0</td>\n",
       "      <td>3306.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>130201.6667</td>\n",
       "      <td>148831.65440</td>\n",
       "      <td>434003.0</td>\n",
       "      <td>69408.0</td>\n",
       "      <td>10000000.0</td>\n",
       "      <td>16330.68406</td>\n",
       "      <td>10000000.0</td>\n",
       "      <td>9968389.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>02/03/2018 08:47:38</td>\n",
       "      <td>118281864</td>\n",
       "      <td>36</td>\n",
       "      <td>83</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>108156.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>134521.0000</td>\n",
       "      <td>79961.04903</td>\n",
       "      <td>191062.0</td>\n",
       "      <td>77980.0</td>\n",
       "      <td>59000000.0</td>\n",
       "      <td>44452.97491</td>\n",
       "      <td>59000000.0</td>\n",
       "      <td>58900000.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49745</td>\n",
       "      <td>6</td>\n",
       "      <td>02/03/2018 08:51:24</td>\n",
       "      <td>96328</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dst Port  Protocol            Timestamp  Flow Duration  Tot Fwd Pkts  \\\n",
       "0     49684         6  02/03/2018 08:47:38            281             2   \n",
       "1       443         6  02/03/2018 08:47:41            250             2   \n",
       "2       443         6  02/03/2018 08:47:40       60860062            15   \n",
       "3       443         6  02/03/2018 08:47:38      118281864            36   \n",
       "4     49745         6  02/03/2018 08:51:24          96328             2   \n",
       "\n",
       "   Tot Bwd Pkts  TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  \\\n",
       "0             1             38.0              0.0             38.0   \n",
       "1             0              0.0              0.0              0.0   \n",
       "2            13            870.0           3306.0            535.0   \n",
       "3            83           1022.0         108156.0            250.0   \n",
       "4             1             38.0              0.0             38.0   \n",
       "\n",
       "   Fwd Pkt Len Min  ...  Fwd Seg Size Min  Active Mean    Active Std  \\\n",
       "0              0.0  ...                20       0.0000       0.00000   \n",
       "1              0.0  ...                20       0.0000       0.00000   \n",
       "2              0.0  ...                20  130201.6667  148831.65440   \n",
       "3              0.0  ...                20  134521.0000   79961.04903   \n",
       "4              0.0  ...                20       0.0000       0.00000   \n",
       "\n",
       "   Active Max  Active Min   Idle Mean     Idle Std    Idle Max    Idle Min  \\\n",
       "0         0.0         0.0         0.0      0.00000         0.0         0.0   \n",
       "1         0.0         0.0         0.0      0.00000         0.0         0.0   \n",
       "2    434003.0     69408.0  10000000.0  16330.68406  10000000.0   9968389.0   \n",
       "3    191062.0     77980.0  59000000.0  44452.97491  59000000.0  58900000.0   \n",
       "4         0.0         0.0         0.0      0.00000         0.0         0.0   \n",
       "\n",
       "    Label  \n",
       "0  Benign  \n",
       "1  Benign  \n",
       "2  Benign  \n",
       "3  Benign  \n",
       "4  Benign  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset into one DataFrame\n",
    "cse_cic_ids2018 = pd.read_csv('Dataset/dataset_combined/CSE-CIC-IDS2018.csv')\n",
    "cse_cic_ids2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1622580\n",
      "Number of columns: 80\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {cse_cic_ids2018.shape[0]}\")\n",
    "print(f\"Number of columns: {cse_cic_ids2018.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Destination Port', 'Flow Duration', 'Total Fwd Packets',\n",
       "       'Total Backward Packets', 'Total Length of Fwd Packets',\n",
       "       'Total Length of Bwd Packets', 'Fwd Packet Length Max',\n",
       "       'Fwd Packet Length Min', 'Fwd Packet Length Mean',\n",
       "       'Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
       "       'Bwd Packet Length Min', 'Bwd Packet Length Mean',\n",
       "       'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s',\n",
       "       'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min',\n",
       "       'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max',\n",
       "       'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std',\n",
       "       'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags',\n",
       "       'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length',\n",
       "       'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s',\n",
       "       'Min Packet Length', 'Max Packet Length', 'Packet Length Mean',\n",
       "       'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count',\n",
       "       'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
       "       'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio',\n",
       "       'Average Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size',\n",
       "       'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate',\n",
       "       'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate',\n",
       "       'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets',\n",
       "       'Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
       "       'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward',\n",
       "       'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\n",
       "       'Idle Std', 'Idle Max', 'Idle Min', 'Label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Columns in the dataset:\")\n",
    "cic_ids2017.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the CIC-IDS2017 dataset, the CSE-CIC-IDS2018 dataset also suffer from the high class imbalance problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Benign                      1347953\n",
       "DDOS attack-HOIC              68801\n",
       "DDoS attacks-LOIC-HTTP        57550\n",
       "DoS attacks-Hulk              46014\n",
       "Bot                           28539\n",
       "FTP-BruteForce                19484\n",
       "SSH-Bruteforce                18485\n",
       "Infilteration                 16160\n",
       "DoS attacks-SlowHTTPTest      14110\n",
       "DoS attacks-GoldenEye          4154\n",
       "DoS attacks-Slowloris          1076\n",
       "DDOS attack-LOIC-UDP            163\n",
       "Brute Force -Web                 59\n",
       "Brute Force -XSS                 25\n",
       "SQL Injection                     7\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Class distribution:')\n",
    "cse_cic_ids2018['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution (normalized):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Benign                      83.074671\n",
       "DDOS attack-HOIC             4.240222\n",
       "DDoS attacks-LOIC-HTTP       3.546820\n",
       "DoS attacks-Hulk             2.835854\n",
       "Bot                          1.758866\n",
       "FTP-BruteForce               1.200804\n",
       "SSH-Bruteforce               1.139235\n",
       "Infilteration                0.995945\n",
       "DoS attacks-SlowHTTPTest     0.869603\n",
       "DoS attacks-GoldenEye        0.256012\n",
       "DoS attacks-Slowloris        0.066314\n",
       "DDOS attack-LOIC-UDP         0.010046\n",
       "Brute Force -Web             0.003636\n",
       "Brute Force -XSS             0.001541\n",
       "SQL Injection                0.000431\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Class distribution (normalized):')\n",
    "cse_cic_ids2018['Label'].value_counts()/cse_cic_ids2018.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows contain null value: \n",
      "Flow Byts/s    5975\n",
      "dtype: int64\n",
      "\n",
      "Rows contain null value (percentage): \n",
      "Flow Byts/s    0.368241\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cse_cic_ids2018_null_count = cse_cic_ids2018.isnull().sum()\n",
    "cse_cic_ids2018_null_count = cse_cic_ids2018_null_count[cse_cic_ids2018_null_count > 0]\n",
    "print(f\"Rows contain null value: \\n{cse_cic_ids2018_null_count}\\n\")\n",
    "\n",
    "cse_cic_ids2018_null_count = cse_cic_ids2018_null_count / cse_cic_ids2018.shape[0] * 100\n",
    "print(f\"Rows contain null value (percentage): \\n{cse_cic_ids2018_null_count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for infinity value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows includes infinity value: 9539; 0.59% of rows\n"
     ]
    }
   ],
   "source": [
    "inf_count = np.isinf(cse_cic_ids2018.iloc[:, 3:-1]).any(axis=1).sum()\n",
    "print(f'Number of rows includes infinity value: {inf_count}; {inf_count/cse_cic_ids2018.shape[0]*100:.2f}% of rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for columns that contain string values\n",
    "\n",
    "The CSE-CIC-IDS2018 dataset contains two column of type `object`, which are the `Timestamp` and `Label`. Since both columns are not storing numerical value, it is very normal and no furter cleaning is needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp    object\n",
       "Label        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cse_cic_ids2018.dtypes[(cse_cic_ids2018.dtypes != 'int64') & (cse_cic_ids2018.dtypes != 'float64')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Benign', 'Bot', 'DoS attacks-SlowHTTPTest', 'DoS attacks-Hulk',\n",
       "       'Brute Force -Web', 'Brute Force -XSS', 'SQL Injection',\n",
       "       'DDoS attacks-LOIC-HTTP', 'Infilteration', 'DoS attacks-GoldenEye',\n",
       "       'DoS attacks-Slowloris', 'FTP-BruteForce', 'SSH-Bruteforce',\n",
       "       'DDOS attack-LOIC-UDP', 'DDOS attack-HOIC'], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cse_cic_ids2018['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          02/03/2018 08:47:38\n",
       "1          02/03/2018 08:47:41\n",
       "2          02/03/2018 08:47:40\n",
       "3          02/03/2018 08:47:38\n",
       "4          02/03/2018 08:51:24\n",
       "                  ...         \n",
       "1622575    28/02/2018 01:48:05\n",
       "1622576    28/02/2018 02:23:17\n",
       "1622577    28/02/2018 04:05:10\n",
       "1622578    28/02/2018 11:12:19\n",
       "1622579    28/02/2018 01:48:05\n",
       "Name: Timestamp, Length: 1622580, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cse_cic_ids2018['Timestamp']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicated column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cse_cic_ids2018.columns[cse_cic_ids2018.columns.value_counts() > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of duplicated rows: 17140\n",
      "1.06% of rows are duplicates\n"
     ]
    }
   ],
   "source": [
    "cse_cic_ids2018_duplicates = cse_cic_ids2018[cse_cic_ids2018.duplicated()]\n",
    "print(f\"number of duplicated rows: {cse_cic_ids2018_duplicates.shape[0]}\")\n",
    "print(f\"{cse_cic_ids2018_duplicates.shape[0]/cse_cic_ids2018.shape[0]*100:.2f}% of rows are duplicates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Dataset cleaning\n",
    "\n",
    "From the analysis in Step 2, it has been discovered that the CSE-CIC-IDS2018 dataset contains a small amount of rows with missing value or infinity value and a small amount of duplicates. These entries will be removed as they only account for a small portion of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1613041, 80)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove rows with null and infinity value\n",
    "with pd.option_context('mode.use_inf_as_na', True):\n",
    "    cse_cic_ids2018 = cse_cic_ids2018.dropna(how='any')\n",
    "\n",
    "cse_cic_ids2018.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1595913, 80)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cse_cic_ids2018 = cse_cic_ids2018.drop_duplicates()\n",
    "cse_cic_ids2018.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Dataset preparation\n",
    "\n",
    "For the CSE-CIC-IDS2018 dataset, there are two goals in this step:\n",
    "1. Just like the CIC-IDS2017 dataset, the 2018 dataset will be downsampled and all attack samples will be labeled as 'malicious'\n",
    "2. The column names of the 2018 dataset will be modified to match that of the 2017 dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Benign                      1338122\n",
       "DDOS attack-HOIC              68628\n",
       "DDoS attacks-LOIC-HTTP        57550\n",
       "DoS attacks-Hulk              45691\n",
       "Bot                           28501\n",
       "SSH-Bruteforce                16312\n",
       "Infilteration                 16034\n",
       "FTP-BruteForce                12368\n",
       "DoS attacks-SlowHTTPTest       7251\n",
       "DoS attacks-GoldenEye          4153\n",
       "DoS attacks-Slowloris          1049\n",
       "DDOS attack-LOIC-UDP            163\n",
       "Brute Force -Web                 59\n",
       "Brute Force -XSS                 25\n",
       "SQL Injection                     7\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the number of sample for each class\n",
    "sample_count_per_class = cse_cic_ids2018['Label'].value_counts()\n",
    "sample_count_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of class after downsampling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Benign                      257791\n",
       "DDOS attack-HOIC             68628\n",
       "DDoS attacks-LOIC-HTTP       57550\n",
       "DoS attacks-Hulk             45691\n",
       "Bot                          28501\n",
       "SSH-Bruteforce               16312\n",
       "Infilteration                16034\n",
       "FTP-BruteForce               12368\n",
       "DoS attacks-SlowHTTPTest      7251\n",
       "DoS attacks-GoldenEye         4153\n",
       "DoS attacks-Slowloris         1049\n",
       "DDOS attack-LOIC-UDP           163\n",
       "Brute Force -Web                59\n",
       "Brute Force -XSS                25\n",
       "SQL Injection                    7\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids2018_attack = downsample_dataset(cse_cic_ids2018, sample_count_per_class[1:], max_sample=100000)\n",
    "num_attack_sample = ids2018_attack.shape[0]\n",
    "ids2018_benign = cse_cic_ids2018[cse_cic_ids2018['Label'] == 'Benign'].sample(n=num_attack_sample).reset_index(drop=True)\n",
    "ids2018_downsampled = pd.concat([ids2018_attack, ids2018_benign])\n",
    "del ids2018_attack\n",
    "del ids2018_benign\n",
    "\n",
    "print('Distribution of class after downsampling')\n",
    "ids2018_downsampled['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution (normalized):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Benign                      50.000000\n",
       "DDOS attack-HOIC            13.310783\n",
       "DDoS attacks-LOIC-HTTP      11.162143\n",
       "DoS attacks-Hulk             8.862024\n",
       "Bot                          5.527928\n",
       "SSH-Bruteforce               3.163803\n",
       "Infilteration                3.109884\n",
       "FTP-BruteForce               2.398842\n",
       "DoS attacks-SlowHTTPTest     1.406372\n",
       "DoS attacks-GoldenEye        0.805497\n",
       "DoS attacks-Slowloris        0.203459\n",
       "DDOS attack-LOIC-UDP         0.031615\n",
       "Brute Force -Web             0.011443\n",
       "Brute Force -XSS             0.004849\n",
       "SQL Injection                0.001358\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Class distribution (normalized):')\n",
    "ids2018_downsampled['Label'].value_counts()/ids2018_downsampled.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relabel the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "benign       257791\n",
       "malicious    257791\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace the label of all attack class to 'malicious'\n",
    "ids2018_downsampled.iloc[ids2018_downsampled['Label'] != 'Benign', -1] = 'malicious'\n",
    "ids2018_downsampled.iloc[ids2018_downsampled['Label'] == 'Benign', -1] = 'benign'\n",
    "ids2018_downsampled['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align the columns name with CIC-IDS2017 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids2018_columns = pd.Series(ids2018_downsampled.columns, dtype='str')\n",
    "ids2017_columns = pd.Series(cic_ids2017.columns, dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = {\n",
    "    \"Dst\": \"Destination\",\n",
    "    \"TotLen\": \"Total Length\",\n",
    "    \"Tot\": \"Total\", \n",
    "    \"Pkt\": \"Packet\",\n",
    "    \"Len\": \"Length\",\n",
    "    \"Cnt\": \"Count\", \n",
    "    \"Var\": \"Variance\",\n",
    "    \"Total Bwd Packets\": \"Total Backward Packets\", \n",
    "    \"Totalal Lengthgth Fwd Packets\": \"Total Length of Fwd Packets\", \n",
    "    \"Totalal Lengthgth Bwd Packets\": \"Total Length of Bwd Packets\", \n",
    "    \"Flow Byts/s\": \"Flow Bytes/s\",\n",
    "    \"Packet Size Avg\": \"Average Packet Size\",\n",
    "    \"Fwd Seg Size Avg\": \"Avg Fwd Segment Size\",\n",
    "    \"Bwd Seg Size Avg\": \"Avg Bwd Segment Size\",\n",
    "    \"Fwd Byts/b Avg\": \"Fwd Avg Bytes/Bulk\",\n",
    "    \"Fwd Packets/b Avg\": \"Fwd Avg Packets/Bulk\",\n",
    "    \"Fwd Blk Rate Avg\": \"Fwd Avg Bulk Rate\", \n",
    "    \"Bwd Byts/b Avg\": \"Bwd Avg Bytes/Bulk\",\n",
    "    \"Bwd Packets/b Avg\": \"Bwd Avg Packets/Bulk\",\n",
    "    \"Bwd Blk Rate Avg\": \"Bwd Avg Bulk Rate\", \n",
    "    \"Init Fwd Win Byts\": \"Init_Win_bytes_forward\", \n",
    "    \"Init Bwd Win Byts\": \"Init_Win_bytes_backward\",\n",
    "    \"Fwd Act Data Packets\": \"act_data_pkt_fwd\",\n",
    "    \"Fwd Seg Size Min\": \"min_seg_size_forward\",\n",
    "    \"Subflow Fwd Byts\": \"Subflow Fwd Bytes\", \n",
    "    \"Subflow Bwd Byts\": \"Subflow Bwd Bytes\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns of the 2018 dataset\n",
    "for original_value in column_mapping:\n",
    "    ids2018_columns = ids2018_columns.replace({original_value: column_mapping[original_value]}, regex=True)\n",
    "\n",
    "ids2018_columns[40] = 'Min Packet Length'\n",
    "ids2018_columns[41] = 'Max Packet Length'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for extra columns in the CSE-CIC-IDS2018 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     Protocol\n",
       "2    Timestamp\n",
       "dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids2018_columns[~ids2018_columns.isin(ids2017_columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the CSE-CIC-IDS2018 dataset miss any column that is in the CIC-IDS2017 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids2017_columns[~ids2017_columns.isin(ids2018_columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the column name of the dataframe and drop the additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Destination Port</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packets</th>\n",
       "      <th>Total Backward Packets</th>\n",
       "      <th>Total Length of Fwd Packets</th>\n",
       "      <th>Total Length of Bwd Packets</th>\n",
       "      <th>Fwd Packet Length Max</th>\n",
       "      <th>Fwd Packet Length Min</th>\n",
       "      <th>Fwd Packet Length Mean</th>\n",
       "      <th>Fwd Packet Length Std</th>\n",
       "      <th>...</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>Active Mean</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80</td>\n",
       "      <td>1794</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>309.0</td>\n",
       "      <td>935.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>178.401233</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>1205</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80</td>\n",
       "      <td>18083</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>298.0</td>\n",
       "      <td>935.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.333333</td>\n",
       "      <td>172.050380</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80</td>\n",
       "      <td>23013</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80</td>\n",
       "      <td>17255</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>326.0</td>\n",
       "      <td>935.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.666667</td>\n",
       "      <td>188.216188</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>malicious</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Destination Port  Flow Duration  Total Fwd Packets  Total Backward Packets  \\\n",
       "0                80           1794                  3                       4   \n",
       "1                80           1205                  2                       0   \n",
       "2                80          18083                  3                       4   \n",
       "3                80          23013                  2                       0   \n",
       "4                80          17255                  3                       4   \n",
       "\n",
       "   Total Length of Fwd Packets  Total Length of Bwd Packets  \\\n",
       "0                        309.0                        935.0   \n",
       "1                          0.0                          0.0   \n",
       "2                        298.0                        935.0   \n",
       "3                          0.0                          0.0   \n",
       "4                        326.0                        935.0   \n",
       "\n",
       "   Fwd Packet Length Max  Fwd Packet Length Min  Fwd Packet Length Mean  \\\n",
       "0                  309.0                    0.0              103.000000   \n",
       "1                    0.0                    0.0                0.000000   \n",
       "2                  298.0                    0.0               99.333333   \n",
       "3                    0.0                    0.0                0.000000   \n",
       "4                  326.0                    0.0              108.666667   \n",
       "\n",
       "   Fwd Packet Length Std  ...  min_seg_size_forward  Active Mean  Active Std  \\\n",
       "0             178.401233  ...                    20          0.0         0.0   \n",
       "1               0.000000  ...                    20          0.0         0.0   \n",
       "2             172.050380  ...                    20          0.0         0.0   \n",
       "3               0.000000  ...                    20          0.0         0.0   \n",
       "4             188.216188  ...                    20          0.0         0.0   \n",
       "\n",
       "   Active Max  Active Min  Idle Mean  Idle Std  Idle Max  Idle Min      Label  \n",
       "0         0.0         0.0        0.0       0.0       0.0       0.0  malicious  \n",
       "1         0.0         0.0        0.0       0.0       0.0       0.0  malicious  \n",
       "2         0.0         0.0        0.0       0.0       0.0       0.0  malicious  \n",
       "3         0.0         0.0        0.0       0.0       0.0       0.0  malicious  \n",
       "4         0.0         0.0        0.0       0.0       0.0       0.0  malicious  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids2018_downsampled.columns = ids2018_columns\n",
    "# drop the additional columns\n",
    "ids2018_downsampled = ids2018_downsampled.drop(['Protocol', 'Timestamp'], axis=1).copy()\n",
    "ids2018_downsampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure the order of the columns are exactly the same for both dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids2018_downsampled = ids2018_downsampled.reindex(columns=ids2017_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_cleaned_dataset(ids2018_downsampled, 'CSE-CIC-IDS2018')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "44c13dc6c3f21bbb987c14254e5f1628aa054b85b2e38852b8ccc1d4b03a22cf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
