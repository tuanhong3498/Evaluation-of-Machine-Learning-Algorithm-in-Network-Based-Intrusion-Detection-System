{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing\n",
    "In this notebook, the goal is to process the dataset so that the dataset is ready to be used for training. To achieve this goal, here are the steps that will be performed in this notebook:\n",
    "1. Combine the CSV files of the dataset into one dataset\n",
    "2. Preliminary analysis of the dataset\n",
    "3. Dataset cleaning - clean missing values, duplicates, etc\n",
    "4. Further processing, includes addressing the high class imbalance problem and rename the columns name (if necessary)\n",
    "5. Save the processed dataset\n",
    "\n",
    "note: The notebook will be seperated into two subsections, where the training dataset and the testing dataset will be processed separately. However, same steps will be applied to both dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages to be used\n",
    "import os       # to create directories and remove files\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# set the random seed to ensure the result is reproducible\n",
    "random.seed(10)\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LUFlow Dataset (2020)\n",
    "\n",
    "For the implementation using the LUFlow dataset, the data collected in July 2020 is being used as training dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Combine the files of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "luflow_csv_files = ('2020.07.01',\n",
    "                    '2020.07.02',\n",
    "                    '2020.07.03',\n",
    "                    '2020.07.04',\n",
    "                    '2020.07.05',\n",
    "                    '2020.07.06',\n",
    "                    '2020.07.07',\n",
    "                    '2020.07.08',\n",
    "                    '2020.07.09',\n",
    "                    '2020.07.10',\n",
    "                    '2020.07.11',\n",
    "                    '2020.07.12',\n",
    "                    '2020.07.13',\n",
    "                    '2020.07.14',\n",
    "                    '2020.07.15',\n",
    "                    '2020.07.16',\n",
    "                    '2020.07.17',\n",
    "                    '2020.07.18',\n",
    "                    '2020.07.19',\n",
    "                    '2020.07.20',\n",
    "                    '2020.07.21',\n",
    "                    '2020.07.22',\n",
    "                    '2020.07.23',\n",
    "                    '2020.07.24',\n",
    "                    '2020.07.25',\n",
    "                    '2020.07.26',\n",
    "                    '2020.07.27',\n",
    "                    '2020.07.28',\n",
    "                    '2020.07.29',\n",
    "                    '2020.07.30',\n",
    "                    '2020.07.31',\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to combine seperate files of a dataset\n",
    "# Besides combining the files, it has two additional functionality:\n",
    "# 1. Replace the replacement character (\\uFFFD) with '-', which is needed for the CIC-IDS2017 dataset\n",
    "# 2. Down sample the dataset. If the reduce_sample_size parameter is set to true, \n",
    "#    only 10% of the dataset will be randomly selected and saved. \n",
    "def combine_csv_files(dataset: str, file_names: tuple, reduce_sample_size: bool = False):\n",
    "\n",
    "    # create a new directory to place the combined dataset\n",
    "    os.makedirs('./Dataset/dataset_combined', exist_ok=True)\n",
    "\n",
    "    # remove the dataset if it already exist\n",
    "    merged_dataset_directory = f'Dataset/dataset_combined/{dataset}.csv'\n",
    "    if os.path.isfile(merged_dataset_directory):\n",
    "        os.remove(merged_dataset_directory)\n",
    "        print(f'original file({merged_dataset_directory}) has been removed')\n",
    "\n",
    "    for (file_index, file_name) in enumerate(file_names):\n",
    "        with open(f\"Dataset/{dataset}/{file_name}.csv\", 'r') as file, open(merged_dataset_directory, 'a') as out_file:\n",
    "            for (line_index, line) in enumerate(file):\n",
    "                \n",
    "                # only the header of the first file will be taken\n",
    "                if 'Label' in line or 'label' in line:\n",
    "                    if file_index != 0 or line_index != 0:\n",
    "                        continue\n",
    "                elif reduce_sample_size:\n",
    "                    if random.randint(1, 10) > 1:\n",
    "                        continue\n",
    "                # replace the replacement character (\\uFFFD) with '-' if exist     \n",
    "                out_file.write(line.replace(' ï¿½ ', '-'))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original file(Dataset/dataset_combined/LUFlow.csv) has been removed\n"
     ]
    }
   ],
   "source": [
    "combine_csv_files(dataset='LUFlow', \n",
    "                    file_names=luflow_csv_files, \n",
    "                    reduce_sample_size=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Preliminary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_ipt</th>\n",
       "      <th>bytes_in</th>\n",
       "      <th>bytes_out</th>\n",
       "      <th>dest_ip</th>\n",
       "      <th>dest_port</th>\n",
       "      <th>entropy</th>\n",
       "      <th>num_pkts_out</th>\n",
       "      <th>num_pkts_in</th>\n",
       "      <th>proto</th>\n",
       "      <th>src_ip</th>\n",
       "      <th>src_port</th>\n",
       "      <th>time_end</th>\n",
       "      <th>time_start</th>\n",
       "      <th>total_entropy</th>\n",
       "      <th>label</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2896</td>\n",
       "      <td>786</td>\n",
       "      <td>9200.0</td>\n",
       "      <td>4.263922</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>786</td>\n",
       "      <td>32862.0</td>\n",
       "      <td>1593574200046275</td>\n",
       "      <td>1593574200046255</td>\n",
       "      <td>12348.318</td>\n",
       "      <td>benign</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>0</td>\n",
       "      <td>10136</td>\n",
       "      <td>786</td>\n",
       "      <td>9200.0</td>\n",
       "      <td>2.710831</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>786</td>\n",
       "      <td>32898.0</td>\n",
       "      <td>1593574199053693</td>\n",
       "      <td>1593574199051282</td>\n",
       "      <td>27476.982</td>\n",
       "      <td>benign</td>\n",
       "      <td>0.002411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>6074</td>\n",
       "      <td>786</td>\n",
       "      <td>9200.0</td>\n",
       "      <td>3.692507</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>786</td>\n",
       "      <td>32902.0</td>\n",
       "      <td>1593574200047203</td>\n",
       "      <td>159357420004712</td>\n",
       "      <td>22428.287</td>\n",
       "      <td>benign</td>\n",
       "      <td>0.000083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2896</td>\n",
       "      <td>786</td>\n",
       "      <td>9200.0</td>\n",
       "      <td>4.803335</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>786</td>\n",
       "      <td>32902.0</td>\n",
       "      <td>1593574201053831</td>\n",
       "      <td>1593574201053819</td>\n",
       "      <td>13910.459</td>\n",
       "      <td>benign</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>786</td>\n",
       "      <td>32898.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>786</td>\n",
       "      <td>9200.0</td>\n",
       "      <td>1593574202069599</td>\n",
       "      <td>1593574202069599</td>\n",
       "      <td>0.000</td>\n",
       "      <td>benign</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    avg_ipt  bytes_in  bytes_out  dest_ip  dest_port   entropy  num_pkts_out  \\\n",
       "0  0.000000         0       2896      786     9200.0  4.263922             2   \n",
       "1  0.285714         0      10136      786     9200.0  2.710831             7   \n",
       "2  0.000000         0       6074      786     9200.0  3.692507             5   \n",
       "3  0.000000         0       2896      786     9200.0  4.803335             2   \n",
       "4  0.000000         0          0      786    32898.0  0.000000             1   \n",
       "\n",
       "   num_pkts_in  proto  src_ip  src_port          time_end        time_start  \\\n",
       "0            0      6     786   32862.0  1593574200046275  1593574200046255   \n",
       "1            3      6     786   32898.0  1593574199053693  1593574199051282   \n",
       "2            0      6     786   32902.0  1593574200047203   159357420004712   \n",
       "3            0      6     786   32902.0  1593574201053831  1593574201053819   \n",
       "4            0      6     786    9200.0  1593574202069599  1593574202069599   \n",
       "\n",
       "   total_entropy   label  duration  \n",
       "0      12348.318  benign  0.000020  \n",
       "1      27476.982  benign  0.002411  \n",
       "2      22428.287  benign  0.000083  \n",
       "3      13910.459  benign  0.000012  \n",
       "4          0.000  benign  0.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the dataset\n",
    "luflow2020 = pd.read_csv('Dataset/dataset_combined/LUFlow.csv')\n",
    "luflow2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2506350\n",
      "Number of columns: 16\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {luflow2020.shape[0]}\")\n",
    "print(f\"Number of columns: {luflow2020.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['avg_ipt', 'bytes_in', 'bytes_out', 'dest_ip', 'dest_port', 'entropy',\n",
       "       'num_pkts_out', 'num_pkts_in', 'proto', 'src_ip', 'src_port',\n",
       "       'time_end', 'time_start', 'total_entropy', 'label', 'duration'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Columns in the dataset:\")\n",
    "luflow2020.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "benign       1396168\n",
       "malicious     905395\n",
       "outlier       204787\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Class distribution:')\n",
    "luflow2020['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the class distribution, we can see that the LUFlow 2020 dataset is slightly imbalance. The benign samples account for 56% of total samples while malicious only account for 36%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution (normalized):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "benign       55.705229\n",
       "malicious    36.124045\n",
       "outlier       8.170726\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Class distribution (normalized):')\n",
    "luflow2020['label'].value_counts()/luflow2020.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null value\n",
    "\n",
    "Count the number of rows include null values in each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows contain null value: \n",
      "dest_port    29268\n",
      "src_port     29268\n",
      "dtype: int64\n",
      "\n",
      "Rows contain null value (percentage): \n",
      "dest_port    1.167754\n",
      "src_port     1.167754\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "luflow2020_null_count = luflow2020.isnull().sum()\n",
    "luflow2020_null_count = luflow2020_null_count[luflow2020_null_count > 0]\n",
    "print(f\"Rows contain null value: \\n{luflow2020_null_count}\\n\")\n",
    "\n",
    "luflow2020_null_count = luflow2020_null_count / luflow2020.shape[0] * 100\n",
    "print(f\"Rows contain null value (percentage): \\n{luflow2020_null_count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for infinity value\n",
    "\n",
    "Check for number of rows that include null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples contains infinity value:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of samples contains infinity value:')\n",
    "np.isinf(luflow2020.iloc[:, :-2]).any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for columns that contain string values\n",
    "\n",
    "Check for columns that is type `object`, which indicate the columns contain string value. The aim is to find if any column contain numeric and alphabetic value. Such column often include string value like '?' to indicate missing value, thus needed to be cleaned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2506350 entries, 0 to 2506349\n",
      "Data columns (total 16 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   avg_ipt        float64\n",
      " 1   bytes_in       int64  \n",
      " 2   bytes_out      int64  \n",
      " 3   dest_ip        int64  \n",
      " 4   dest_port      float64\n",
      " 5   entropy        float64\n",
      " 6   num_pkts_out   int64  \n",
      " 7   num_pkts_in    int64  \n",
      " 8   proto          int64  \n",
      " 9   src_ip         int64  \n",
      " 10  src_port       float64\n",
      " 11  time_end       int64  \n",
      " 12  time_start     int64  \n",
      " 13  total_entropy  float64\n",
      " 14  label          object \n",
      " 15  duration       float64\n",
      "dtypes: float64(6), int64(9), object(1)\n",
      "memory usage: 306.0+ MB\n"
     ]
    }
   ],
   "source": [
    "luflow2020.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can see that only the 'label' column is of type `object`. As the 'label' column store the label of each sample using `string`, it has no problem with the column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for duplicated column\n",
    "luflow2020.columns[luflow2020.columns.value_counts() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "508 rows are duplicates\n",
      "0.02% of rows are duplicates\n"
     ]
    }
   ],
   "source": [
    "luflow_duplicates = luflow2020[luflow2020.duplicated()]\n",
    "print(f\"{luflow_duplicates.shape[0]} rows are duplicates\")\n",
    "print(f\"{luflow_duplicates.shape[0]/luflow2020.shape[0]*100:.2f}% of rows are duplicates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Dataset cleaning\n",
    "\n",
    "From the analysis in Step 2, it has been discovered that the dataset contains a small amount of missing value and duplicates. Since those problematic rows only account for a small portion of the entire dataset, those rows are simply removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2477082, 16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove rows contain missing value\n",
    "luflow2020 = luflow2020.dropna(how='any')\n",
    "luflow2020.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2476585, 16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luflow2020 = luflow2020.drop_duplicates()\n",
    "luflow2020.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Dataset preparation\n",
    "\n",
    "In this step, the main goal is to prepare the dataset to be ready for training. For the LUFlow dataset, there are two tasks to be completed:\n",
    "* Remove outlier - The LUFlow dataset contains an \"outlier\" class. In the outlier class, it contains samples that are malicious, but could also be benign. Hence, the outliers are not meaningful as they sits in the grey area between malicious and benign class. \n",
    "* Balance the class distribution - The benign samples are slightly more than the malicious samples, which could result in the models bias towards the benign samples. Since we have a large number of samples, we will downsampling the benign samples, so that the ratio between benign and malicious samples is 1:1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "benign       879740\n",
       "malicious    879740\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack = luflow2020[luflow2020['label']=='malicious']\n",
    "benign = luflow2020[luflow2020['label']=='benign'].sample(n=len(attack)).reset_index(drop=True)\n",
    "\n",
    "luflow2020_exclude_outlier = pd.concat([attack, benign])\n",
    "del attack\n",
    "del benign\n",
    "\n",
    "luflow2020_exclude_outlier['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save the cleaned dataset\n",
    "def save_cleaned_dataset(dataframe: pd.DataFrame,dataset: str, tag: str = \"\"):\n",
    "    # create a new directory to save the cleaned dataset\n",
    "    os.makedirs('./Dataset/dataset_cleaned', exist_ok=True)\n",
    "\n",
    "    if not(tag == \"\"):\n",
    "        tag = \"_\" + tag\n",
    "\n",
    "    dataframe.to_csv(f'Dataset/dataset_cleaned/{dataset}{tag}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_cleaned_dataset(dataframe=luflow2020_exclude_outlier, dataset='LUFlow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LUFlow Dataset (2021)\n",
    "\n",
    "For the implementation using the LUFlow dataset, the data collected in January 2021 is being used as testing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "luflow2021_csv_files = ('2021.01.01',\n",
    "                        '2021.01.02',\n",
    "                        '2021.01.03',\n",
    "                        '2021.01.04',\n",
    "                        '2021.01.05',\n",
    "                        '2021.01.06',\n",
    "                        '2021.01.07',\n",
    "                        '2021.01.08',\n",
    "                        '2021.01.09',\n",
    "                        '2021.01.10',\n",
    "                        '2021.01.11',\n",
    "                        '2021.01.12',\n",
    "                        '2021.01.13',\n",
    "                        '2021.01.14',\n",
    "                        '2021.01.15',\n",
    "                        '2021.01.17',\n",
    "                        '2021.01.18',\n",
    "                        '2021.01.19',\n",
    "                        '2021.01.20',\n",
    "                        '2021.01.22',\n",
    "                        '2021.01.23',\n",
    "                        '2021.01.24',\n",
    "                        '2021.01.25',\n",
    "                        '2021.01.26',\n",
    "                        '2021.01.27',\n",
    "                        '2021.01.28',\n",
    "                        '2021.01.29',\n",
    "                        '2021.01.30',\n",
    "                        '2021.01.31',\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original file(Dataset/dataset_combined/LUFlow_2021.csv) has been removed\n"
     ]
    }
   ],
   "source": [
    "combine_csv_files(dataset='LUFlow_2021', \n",
    "                    file_names=luflow2021_csv_files,\n",
    "                    reduce_sample_size=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_ipt</th>\n",
       "      <th>bytes_in</th>\n",
       "      <th>bytes_out</th>\n",
       "      <th>dest_ip</th>\n",
       "      <th>dest_port</th>\n",
       "      <th>entropy</th>\n",
       "      <th>num_pkts_out</th>\n",
       "      <th>num_pkts_in</th>\n",
       "      <th>proto</th>\n",
       "      <th>src_ip</th>\n",
       "      <th>src_port</th>\n",
       "      <th>time_end</th>\n",
       "      <th>time_start</th>\n",
       "      <th>total_entropy</th>\n",
       "      <th>label</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>907.312500</td>\n",
       "      <td>754</td>\n",
       "      <td>32742</td>\n",
       "      <td>786</td>\n",
       "      <td>9092.0</td>\n",
       "      <td>1.111004</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>786</td>\n",
       "      <td>57148.0</td>\n",
       "      <td>1609467701725488</td>\n",
       "      <td>1609467672683006</td>\n",
       "      <td>37214.200</td>\n",
       "      <td>benign</td>\n",
       "      <td>29.042482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>786</td>\n",
       "      <td>40010.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>786</td>\n",
       "      <td>9300.0</td>\n",
       "      <td>1609467690746479</td>\n",
       "      <td>1609467690746443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>benign</td>\n",
       "      <td>0.000036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>174</td>\n",
       "      <td>29008</td>\n",
       "      <td>786</td>\n",
       "      <td>9092.0</td>\n",
       "      <td>1.127009</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>786</td>\n",
       "      <td>57148.0</td>\n",
       "      <td>1609467711729621</td>\n",
       "      <td>160946771172828</td>\n",
       "      <td>32888.370</td>\n",
       "      <td>benign</td>\n",
       "      <td>0.001341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>786</td>\n",
       "      <td>40012.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>786</td>\n",
       "      <td>9300.0</td>\n",
       "      <td>1609467690746484</td>\n",
       "      <td>1609467690746469</td>\n",
       "      <td>0.000</td>\n",
       "      <td>benign</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1502.333333</td>\n",
       "      <td>664</td>\n",
       "      <td>13772</td>\n",
       "      <td>786</td>\n",
       "      <td>9200.0</td>\n",
       "      <td>2.341442</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>786</td>\n",
       "      <td>56330.0</td>\n",
       "      <td>1609467732753774</td>\n",
       "      <td>160946772373801</td>\n",
       "      <td>33801.062</td>\n",
       "      <td>benign</td>\n",
       "      <td>9.015764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       avg_ipt  bytes_in  bytes_out  dest_ip  dest_port   entropy  \\\n",
       "0   907.312500       754      32742      786     9092.0  1.111004   \n",
       "1     0.000000         0          0      786    40010.0  0.000000   \n",
       "2     0.000000       174      29008      786     9092.0  1.127009   \n",
       "3     0.000000         0          0      786    40012.0  0.000000   \n",
       "4  1502.333333       664      13772      786     9200.0  2.341442   \n",
       "\n",
       "   num_pkts_out  num_pkts_in  proto  src_ip  src_port          time_end  \\\n",
       "0            32           20      6     786   57148.0  1609467701725488   \n",
       "1             1            1      6     786    9300.0  1609467690746479   \n",
       "2             7            5      6     786   57148.0  1609467711729621   \n",
       "3             1            1      6     786    9300.0  1609467690746484   \n",
       "4             6            5      6     786   56330.0  1609467732753774   \n",
       "\n",
       "         time_start  total_entropy   label   duration  \n",
       "0  1609467672683006      37214.200  benign  29.042482  \n",
       "1  1609467690746443          0.000  benign   0.000036  \n",
       "2   160946771172828      32888.370  benign   0.001341  \n",
       "3  1609467690746469          0.000  benign   0.000015  \n",
       "4   160946772373801      33801.062  benign   9.015764  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luflow2021 = pd.read_csv('Dataset/dataset_combined/LUFlow_2021.csv')\n",
    "luflow2021.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2699669\n",
      "Number of columns: 16\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {luflow2021.shape[0]}\")\n",
    "print(f\"Number of columns: {luflow2021.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['avg_ipt', 'bytes_in', 'bytes_out', 'dest_ip', 'dest_port', 'entropy',\n",
       "       'num_pkts_out', 'num_pkts_in', 'proto', 'src_ip', 'src_port',\n",
       "       'time_end', 'time_start', 'total_entropy', 'label', 'duration'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Columns in the dataset:\")\n",
    "luflow2021.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "benign       1638952\n",
       "malicious     591372\n",
       "outlier       469345\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Class distribution')\n",
    "luflow2021['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the class distribution, we can see that the LUFlow 2021 dataset is also imbalance. The benign samples account for 56% of total samples while malicious only account for 36%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution (normalized):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "benign       56.359422\n",
       "malicious    35.522302\n",
       "outlier       8.118276\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Class distribution (normalized):')\n",
    "luflow2020['label'].value_counts()/luflow2020.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows contain null value: \n",
      "dest_port    33810\n",
      "src_port     33810\n",
      "dtype: int64\n",
      "\n",
      "Rows contain null value (percentage): \n",
      "dest_port    1.252376\n",
      "src_port     1.252376\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "luflow2021_null_count = luflow2021.isnull().sum()\n",
    "luflow2021_null_count = luflow2021_null_count[luflow2021_null_count > 0]\n",
    "print(f\"Rows contain null value: \\n{luflow2021_null_count}\\n\")\n",
    "\n",
    "luflow2021_null_count = luflow2021_null_count / luflow2021.shape[0] * 100\n",
    "print(f\"Rows contain null value (percentage): \\n{luflow2021_null_count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for infinity value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples contains infinity value:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of samples contains infinity value:')\n",
    "np.isinf(luflow2021.iloc[:, :-2]).any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for columns that contain string values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2699669 entries, 0 to 2699668\n",
      "Data columns (total 16 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   avg_ipt        float64\n",
      " 1   bytes_in       int64  \n",
      " 2   bytes_out      int64  \n",
      " 3   dest_ip        int64  \n",
      " 4   dest_port      float64\n",
      " 5   entropy        float64\n",
      " 6   num_pkts_out   int64  \n",
      " 7   num_pkts_in    int64  \n",
      " 8   proto          int64  \n",
      " 9   src_ip         int64  \n",
      " 10  src_port       float64\n",
      " 11  time_end       int64  \n",
      " 12  time_start     int64  \n",
      " 13  total_entropy  float64\n",
      " 14  label          object \n",
      " 15  duration       float64\n",
      "dtypes: float64(6), int64(9), object(1)\n",
      "memory usage: 329.5+ MB\n"
     ]
    }
   ],
   "source": [
    "luflow2021.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the LUFlow 2020 dataset, only the 'label' column is of type `object`, so there will be no cleaning needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for duplicated column\n",
    "luflow2021.columns[luflow2021.columns.value_counts() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290 rows are duplicates\n",
      "0.01% of rows are duplicates\n"
     ]
    }
   ],
   "source": [
    "luflow2021_duplicates = luflow2021[luflow2021.duplicated()]\n",
    "print(f\"{luflow2021_duplicates.shape[0]} rows are duplicates\")\n",
    "print(f\"{luflow2021_duplicates.shape[0]/luflow2021.shape[0]*100:.2f}% of rows are duplicates\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Dataset cleaning\n",
    "\n",
    "Just like the LUFlow 2020 dataset, the LUFlow 2021 dataset contains a small amount of missing value and duplicated rows. Those problematic rows in this dataset are also removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2665859, 16)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luflow2021 = luflow2021.dropna(how='any')\n",
    "luflow2021.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2665569, 16)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "luflow2021 = luflow2021.drop_duplicates()\n",
    "luflow2021.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Dataset preparation\n",
    "\n",
    "Same as before, the outliers are removed and the class distribution are balanced in this step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "benign       569003\n",
       "malicious    569003\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack = luflow2021[luflow2021['label'] == 'malicious']\n",
    "benign = luflow2021[luflow2021['label'] == 'benign'].sample(n=len(attack)).reset_index(drop=True)\n",
    "\n",
    "luflow2021_exclude_outlier = pd.concat([attack, benign])\n",
    "del attack\n",
    "del benign\n",
    "\n",
    "luflow2021_exclude_outlier['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_cleaned_dataset(dataframe=luflow2021_exclude_outlier, dataset='LUFlow2021')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "44c13dc6c3f21bbb987c14254e5f1628aa054b85b2e38852b8ccc1d4b03a22cf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
